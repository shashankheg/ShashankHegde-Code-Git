{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e3eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (3.3.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shashank\\anaconda_new\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28fabea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f4f64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb8824b4c164743a7f8deaf3c8338d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd153f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c97f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\" , split= \"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c19c7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0ea51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9d9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d133d017492d1400aabc22',\n",
       " 'title': 'The_Legend_of_Zelda:_Twilight_Princess',\n",
       " 'context': 'Special bundles of the game contain a Wolf Link Amiibo figurine, which unlocks a Wii U-exclusive dungeon called the \"Cave of Shadows\" and can carry data over to the upcoming 2016 Zelda game. Other Zelda-related Amiibo figurines have distinct functions: Link and Toon Link replenish arrows, Zelda and Sheik restore Link\\'s health, and Ganondorf causes Link to take twice as much damage.',\n",
       " 'question': 'What characters will be able to replenish arrows?',\n",
       " 'answers': {'text': ['Link and Toon'], 'answer_start': [253]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78d3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5982d563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d4cee32ccc5a1400d8324a',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'On February 6, 2016, one day before her performance at the Super Bowl, Beyoncé released a new single exclusively on music streaming service Tidal called \"Formation\".',\n",
       " 'question': 'When did Beyoncé release Formation?',\n",
       " 'answers': {'text': ['February 6, 2016'], 'answer_start': [3]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089320fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53697137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'title', 'context', 'question', 'answers']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2a3f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57758c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'title', 'context', 'question', 'answers']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05d19487",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def preprocess_function(examples):\n",
    "...     questions = [q.strip() for q in examples[\"question\"]]\n",
    "...     inputs = tokenizer(\n",
    "...         questions,\n",
    "...         examples[\"context\"],\n",
    "...         max_length=384,\n",
    "...         truncation=\"only_second\",\n",
    "...         return_offsets_mapping=True,\n",
    "...         padding=\"max_length\",\n",
    "...     )\n",
    "\n",
    "...     offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "...     answers = examples[\"answers\"]\n",
    "...     start_positions = []\n",
    "...     end_positions = []\n",
    "\n",
    "...     for i, offset in enumerate(offset_mapping):\n",
    "...         answer = answers[i]\n",
    "...         start_char = answer[\"answer_start\"][0]\n",
    "...         end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "...         sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "...         # Find the start and end of the context\n",
    "...         idx = 0\n",
    "...         while sequence_ids[idx] != 1:\n",
    "...             idx += 1\n",
    "...         context_start = idx\n",
    "...         while sequence_ids[idx] == 1:\n",
    "...             idx += 1\n",
    "...         context_end = idx - 1\n",
    "\n",
    "...         # If the answer is not fully inside the context, label it (0, 0)\n",
    "...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "...             start_positions.append(0)\n",
    "...             end_positions.append(0)\n",
    "...         else:\n",
    "...             # Otherwise it's the start and end token positions\n",
    "...             idx = context_start\n",
    "...             while idx <= context_end and offset[idx][0] <= start_char:\n",
    "...                 idx += 1\n",
    "...             start_positions.append(idx - 1)\n",
    "\n",
    "...             idx = context_end\n",
    "...             while idx >= context_start and offset[idx][1] >= end_char:\n",
    "...                 idx -= 1\n",
    "...             end_positions.append(idx + 1)\n",
    "\n",
    "...     inputs[\"start_positions\"] = start_positions\n",
    "...     inputs[\"end_positions\"] = end_positions\n",
    "...     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "691c2466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc321489aed4bf783a3c17e6bd10041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab364b698994d82b83fd3348277d45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2323a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5a4c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd3567a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ">>> from transformers import DefaultDataCollator\n",
    "\n",
    ">>> data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fef3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    ">>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    ">>> model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5da9b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 3:21:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.410628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.789800</td>\n",
       "      <td>1.771497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.789800</td>\n",
       "      <td>1.644778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.3578301595052085, metrics={'train_runtime': 12107.7907, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'total_flos': 1175877900288000.0, 'train_loss': 2.3578301595052085, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_qa_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True\n",
    ")\n",
    "trainer =Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator= data_collator ,\n",
    "    \n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9d8a150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee7be0b58154eceac586e584736524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1745367033.SHASHHEG-5CHDHW3.17268.2:   0%|          | 0.00/6.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shashankheg/my_qa_model/commit/e6de9130e98df2b02e7cb173b56abec9b28ac310', commit_message='End of training', commit_description='', oid='e6de9130e98df2b02e7cb173b56abec9b28ac310', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shashankheg/my_qa_model', endpoint='https://huggingface.co', repo_type='model', repo_id='shashankheg/my_qa_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55311345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize function to finetune the tensorflow model\n",
    "from sched import scheduler\n",
    "from transformers import create_optimizer\n",
    "batch_size =16\n",
    "num_epochs = 2\n",
    "total_train_steps =((len(tokenized_squad['train']))/batch_size)*num_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=total_train_steps,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "178fde28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedbfd401a394098814289cec859872e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d975295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    ">>> from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    ">>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ce9ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert your datasets to the tf.data.Dataset format with [~transformers.TFPreTrainedModel.prepare_tf_dataset]:\n",
    ">>> tf_train_set = model.prepare_tf_dataset(\n",
    "...     tokenized_squad[\"train\"],\n",
    "...     shuffle=True,\n",
    "...     batch_size=16,\n",
    "...     collate_fn=data_collator,\n",
    "... )\n",
    "\n",
    ">>> tf_validation_set = model.prepare_tf_dataset(\n",
    "...     tokenized_squad[\"test\"],\n",
    "...     shuffle=False,\n",
    "...     batch_size=16,\n",
    "...     collate_fn=data_collator,\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02754155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dc23616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/shashankheg/my_qa_model into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590a5fa37af9446d97f6ccd9f438c0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file model.safetensors:   0%|          | 14.7k/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2475466d656a4df99de6ed4978e6304d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Apr22_18-06-09_SHASHHEG-5CHDHW3/events.out.tfevents.1745366771.SHASHHEG-5CHDHW3.17268.0: 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c21f4d516f4f7296aebb06c2841d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Apr22_18-10-33_SHASHHEG-5CHDHW3/events.out.tfevents.1745367033.SHASHHEG-5CHDHW3.17268.2: 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f506fae0034c4f139dc70900a73bcc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 5.18k/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c4259161044ba091f2d88fbba3a33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Apr22_18-09-24_SHASHHEG-5CHDHW3/events.out.tfevents.1745366965.SHASHHEG-5CHDHW3.17268.1: 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a4e0344be94d01bd78e3d585eaf8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Apr22_18-06-09_SHASHHEG-5CHDHW3/events.out.tfevents.1745366771.SHASHHEG-5CHDHW3.17268.0: 100%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423690c9bb9b43dc81decbed42853c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Apr22_18-10-33_SHASHHEG-5CHDHW3/events.out.tfevents.1745367033.SHASHHEG-5CHDHW3.17268.2: 100%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338be3e70aab48cbb85e00aefb963b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin: 100%|##########| 5.18k/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b64330c45144d9b0e21dea85bfe5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Apr22_18-09-24_SHASHHEG-5CHDHW3/events.out.tfevents.1745366965.SHASHHEG-5CHDHW3.17268.1: 100%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ">>> from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    ">>> callback = PushToHubCallback(\n",
    "...     output_dir=\"/my_qa_model\",\n",
    "...     tokenizer=tokenizer,\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20f36a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x28e720e03b0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1cd4864b",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> question = 'When did Beyoncé release Formation?'\n",
    ">>> context = 'On February 6, 2016, one day before her performance at the Super Bowl, Beyoncé released a new single exclusively on music streaming service Tidal called \"Formation\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e3437ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.3482300341129303,\n",
       " 'start': 3,\n",
       " 'end': 19,\n",
       " 'answer': 'February 6, 2016'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from transformers import pipeline\n",
    "\n",
    ">>> question_answerer = pipeline(\"question-answering\", model=\"my_qa_model\")\n",
    ">>> question_answerer(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "190a6522",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from transformers import AutoTokenizer\n",
    "\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"my_qa_model\")\n",
    ">>> inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6554a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.7567, -0.9099, -0.9439, -3.1180, -4.5906, -3.8437, -3.1428, -4.4424,\n",
       "         -4.6074, -4.1999, -2.6713, -2.7966,  4.8902,  0.3930, -2.1254, -3.8282,\n",
       "         -2.3827, -2.0333,  1.2862, -3.1707,  5.4366, -2.3450,  3.0304, -3.9342,\n",
       "         -2.1986,  5.0488, -3.2159, -4.2492, -3.9939, -4.1999]]), end_logits=tensor([[-3.0765, -2.2695, -0.7537, -3.7772, -1.5518, -4.0324, -2.5278, -3.8324,\n",
       "         -4.6308, -1.2707, -2.8741, -3.9547,  0.7762,  3.4770,  0.8205, -3.9714,\n",
       "         -4.3026, -3.2251,  0.1135, -4.1172,  3.8016,  1.0495, -0.1765,  0.5171,\n",
       "         -3.9315,  5.0966, -2.7018,  0.8294, -0.7932, -1.2704]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import torch\n",
    ">>> from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    ">>> model = AutoModelForQuestionAnswering.from_pretrained(\"my_qa_model\")\n",
    ">>> with torch.no_grad():\n",
    "...     outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7bace043",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> answer_start_index = outputs.start_logits.argmax()\n",
    ">>> answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b66a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'176 billion parameters and can generate text in 46 languages natural languages and 13'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    ">>> tokenizer.decode(predict_answer_tokens)\n",
    "'176 billion parameters and can generate text in 46 languages natural languages and 13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7922bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from transformers import AutoTokenizer\n",
    "\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"my_qa_model\")\n",
    ">>> inputs = tokenizer(question, context, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89081825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    ">>> from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    ">>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_qa_model\")\n",
    ">>> outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3c004e6",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    ">>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93bda2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'176 billion parameters and can generate text in 46 languages natural languages and 13'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    ">>> tokenizer.decode(predict_answer_tokens)\n",
    "'176 billion parameters and can generate text in 46 languages natural languages and 13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc1f9b8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (3793779711.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[89], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    'context': 'On February 6, 2016, one day before her performance at the Super Bowl, Beyoncé released a new single exclusively on music streaming service Tidal called \"Formation\".',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "'context': 'On February 6, 2016, one day before her performance at the Super Bowl, Beyoncé released a new single exclusively on music streaming service Tidal called \"Formation\".',\n",
    " 'question': 'When did Beyoncé release Formation?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5c3a1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question: 'What characters will be able to replenish arrows?'\n",
    "context: 'Special bundles of the game contain a Wolf Link Amiibo figurine, which unlocks a Wii U-exclusive dungeon called the \"Cave of Shadows\" and can carry data over to the upcoming 2016 Zelda game. Other Zelda-related Amiibo figurines have distinct functions: Link and Toon Link replenish arrows, Zelda and Sheik restore Link\\'s health, and Ganondorf causes Link to take twice as much damage.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca3a9921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    ">>> from transformers import pipeline\n",
    "\n",
    ">>> question_answerer = pipeline(\"question-answering\", model=\"my_qa_model\")\n",
    "result = question_answerer(question=question, context=context)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
